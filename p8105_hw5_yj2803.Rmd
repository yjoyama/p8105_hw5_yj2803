---
title: "Homework 5"
author: "Yuki Joyama"
date: "2023-11-07"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1 
```{r message=FALSE}
library(tidyverse)

# dataset preparation
df_homicide = read_csv("./data/homicide-data.csv") |> 
  janitor::clean_names() 
```

The homicide data collected by the Washington Post has `r ncol(df_homicide)` columns and `r nrow(df_homicide)` rows. The data set comprises information of homicide such as incident dates, victim names, their race, gender, location, etc.

```{r message=FALSE}
df_homicide = df_homicide |>  
  mutate(
    city_state = paste(city, state, sep = ", ") # create a city_state variable
  ) 

# the total number of homicides within cities
df_n_homicide = df_homicide |> 
  group_by(city_state) |> 
  summarise(n_homicide = n()) |> 
  arrange(desc(n_homicide))

# the number of unsolved homicides within cities
df_n_unsolved_homicide = df_homicide |> 
  filter(disposition == "Closed without arrest" | disposition == "Open/No arrest") |> 
  group_by(city_state) |> 
  summarise(n_unsolved_homicide = n()) |> 
  arrange(desc(n_unsolved_homicide))

# combine two data sets
df_n_homicide = left_join(df_n_homicide, df_n_unsolved_homicide) |> 
  drop_na()

df_n_homicide
```

The above outputs show the total number of homicides and unsolved homicides within cities. We can see that Chicago, IL has the most homicide cases. 

```{r}
# estimate the proportion of homicides that are unsolved in Baltimore, MD
prop.test(x = filter(df_n_homicide, city_state == "Baltimore, MD") |> pull(n_unsolved_homicide), n = filter(df_n_homicide, city_state == "Baltimore, MD") |> pull(n_homicide)) |> 
  broom::tidy() |> 
  knitr::kable(digits = 2)
```

The estimated proportion of homicides that are unsolved in Baltimore, MD is 0.65 with 95% confidence interval 0.63 - 0.66.

```{r}
# run prop.test for each of the cities
unsolved_prop_test = df_n_homicide |> 
  mutate(
    prop_test = purrr::map2(pull(df_n_homicide, n_unsolved_homicide), pull(df_n_homicide, n_homicide), \(x, y) prop.test(x = x, n = y)),
    tidy_test = purrr::map(prop_test, broom::tidy) 
  ) |> 
  unnest(tidy_test) |> 
  select(city_state, estimate, conf.low, conf.high)

# create a plot that shows the estimates and CIs for each city
unsolved_prop_test |> 
  mutate(city_state = fct_reorder(city_state, estimate)) |> 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(x = city_state, ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90))
```

The city with the lowest estimate of the proportion for unsolved homicide is Richmond, VA. The city with the highest estimate is Chicago, IL.

# Problem 2

```{r message=FALSE}
# save file names as a list 
df_long = tibble(files = list.files(path = "./data/")) |>
  filter(files != "homicide-data.csv") |> # remove homicide data from problem 1
  mutate(
    data = purrr::map_df(files, ~read_csv(file.path("./data/", .))), # import csv files
    arm = case_when(
      str_starts(files, "con_") ~ "control",
      str_starts(files, "exp_") ~ "experiment"
    )
  ) |> 
  group_by(arm) |> 
  mutate(id = row_number()) |> 
  ungroup() |> 
  unnest(data) |> 
  pivot_longer(
    cols = starts_with("week"),
    names_to = "week",
    values_to = "observations"
  ) |> 
  mutate(
    week = as.numeric(str_remove(week, "week_")), # rewrite week_* to * indicating the number of the week
    id = as.factor(id)
  ) |> 
  select(-files)
```

```{r}
# visualization - spaghetti plot 
# obs on each subject over time by groups
df_long |> 
  ggplot(aes(x = week, y = observations, group = id, color = id)) +
  geom_line() +
  facet_grid(~arm) +
  labs(
    title = "Longitudinal trend in control and experimental groups"
  )
```

The plots illustrate that the observed values for the experimental group increased significantly over time, whereas the values for the control group remained at the same level.

# Problem 3

```{r}
# set the design elements
n = 30
sigma = 5
mu = 0

# 5000 datasets from the model x ~ norm(mu, sigma)
# alpha = 0.05, two-sided one-sample t-test
sim_df_0 <- tibble(iter = 1:5000, mu = 0) |> 
  mutate(
    df_sim = map(iter, \(i) rnorm(n, mean = mu, sd = sigma)),
    mu_hat = map_dbl(df_sim, mean), # calculate mu_hat for each dataset
    p_value = map(df_sim, \(df) t.test(df, alternative = "two.sided", mu = 0, conf.level = .95) |> broom::tidy() |> pull(p.value)) # perform t-test, convert outcome to tibble and extract p-value
  ) |> 
  unnest(p_value)

# repeat the above for mu = {1, 2, 3, 4, 5, 6}
output = list()

for (mu_value in 1:6) {
  sim_results = tibble(iter = 1:5000, mu = mu_value) |> 
  mutate(
    df_sim = map(iter, \(i) rnorm(n, mean = mu_value, sd = sigma)),
    mu_hat = map_dbl(df_sim, mean), # calculate mu_hat for each dataset
    p_value = map(df_sim, \(df) t.test(df, alternative = "two.sided", mu = 0, conf.level = .95) |> broom::tidy() |> pull(p.value)) # perform t-test, convert outcome to tibble and extract p-value
  ) |> 
  unnest(p_value)
  
  output[[mu_value]] = sim_results
}

sim_df_1_6 = bind_rows(output) 

# merge sim_0 and sim_1_6
sim_df = bind_rows(sim_df_0, sim_df_1_6)
```

```{r}
# plot: x = true value of mu, y = the proportion of times the null was rejected

sim_df |> 
  group_by(mu) |> 
  summarise(prop = mean(p_value < .05)) |> 
  ggplot(aes(x = mu, y = prop)) +
  geom_point() +
  geom_line() +
  labs(
    title = "The power of one-sample t-test",
    x = "The true value of mean",
    y = "The power of the test"
  ) 

```

This graph shows that the power of the test increases as the effect size (the difference between $\hat{\mu}$ and $\mu$) increases. 

```{r}
p_mu = sim_df |> 
  group_by(mu) |> 
  summarise(
    mu_hat = mean(mu_hat)
  ) |> 
  ggplot(aes(x = mu, y = mu_hat)) +
  geom_point() +
  geom_line() +
  labs(
    title = "The association between the true mean and the sample mean",
    x = "The true value of mean",
    y = "The average estimate of the sample mean"
  ) 

# calculate the average of mu_hat when p_value < 0.05
mu_alt = sim_df |> 
  filter(p_value < .05) |> 
  group_by(mu) |> 
  summarise(
    mu_hat_alt = mean(mu_hat)
  ) 

# combine two plots
p_mu + 
  geom_point(data = mu_alt, aes(x = mu, y = mu_hat_alt), color = "red")
```

The black points shows the true mean $\mu$ vs. the average estimate of the sample means ($\hat{\mu}_0$) and the red ones stand for the true mean $\mu$ vs. the average estimate of the sample means where the null hypothesis was rejected ($\hat{\mu}_1$). 

We can see that $\hat{\mu}_0$ is almost equal to $\mu$. However, differences are seen between $\hat{\mu}_1$ and $\mu$ when the effect size is small, and they become similar as the effect size increases.


